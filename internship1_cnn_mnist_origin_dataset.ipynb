{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Họ tên: Võ Minh Tung\n",
    "- Mã số học viên: 2370345"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zmFm1gK46CoC"
   },
   "source": [
    "1. Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "u0ktuKIV44i6",
    "outputId": "89b1d51d-4404-443a-a8bc-bbeb99ca9c9a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.Requirement already satisfied: transformers in e:\\software\\python\\lib\\site-packages (4.46.0)\n",
      "Requirement already satisfied: datasets in e:\\software\\python\\lib\\site-packages (3.1.0)\n",
      "Requirement already satisfied: torch in e:\\software\\python\\lib\\site-packages (2.5.1)\n",
      "Requirement already satisfied: torchvision in e:\\software\\python\\lib\\site-packages (0.20.1)\n",
      "Requirement already satisfied: evaluate in e:\\software\\python\\lib\\site-packages (0.4.3)\n",
      "Requirement already satisfied: filelock in e:\\software\\python\\lib\\site-packages (from transformers) (3.16.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in e:\\software\\python\\lib\\site-packages (from transformers) (0.26.2)\n",
      "Requirement already satisfied: numpy>=1.17 in e:\\software\\python\\lib\\site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\vomtu\\appdata\\roaming\\python\\python312\\site-packages (from transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in e:\\software\\python\\lib\\site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in e:\\software\\python\\lib\\site-packages (from transformers) (2024.9.11)\n",
      "Requirement already satisfied: requests in e:\\software\\python\\lib\\site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in e:\\software\\python\\lib\\site-packages (from transformers) (0.4.5)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in e:\\software\\python\\lib\\site-packages (from transformers) (0.20.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in e:\\software\\python\\lib\\site-packages (from transformers) (4.66.5)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in e:\\software\\python\\lib\\site-packages (from datasets) (17.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in e:\\software\\python\\lib\\site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in e:\\software\\python\\lib\\site-packages (from datasets) (2.2.3)\n",
      "Requirement already satisfied: xxhash in e:\\software\\python\\lib\\site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in e:\\software\\python\\lib\\site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in e:\\software\\python\\lib\\site-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets) (2024.9.0)\n",
      "Requirement already satisfied: aiohttp in e:\\software\\python\\lib\\site-packages (from datasets) (3.10.10)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in e:\\software\\python\\lib\\site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: networkx in e:\\software\\python\\lib\\site-packages (from torch) (3.4.1)\n",
      "Requirement already satisfied: jinja2 in e:\\software\\python\\lib\\site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: setuptools in e:\\software\\python\\lib\\site-packages (from torch) (75.1.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in e:\\software\\python\\lib\\site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in e:\\software\\python\\lib\\site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in e:\\software\\python\\lib\\site-packages (from torchvision) (10.4.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in e:\\software\\python\\lib\\site-packages (from aiohttp->datasets) (2.4.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in e:\\software\\python\\lib\\site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in e:\\software\\python\\lib\\site-packages (from aiohttp->datasets) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in e:\\software\\python\\lib\\site-packages (from aiohttp->datasets) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in e:\\software\\python\\lib\\site-packages (from aiohttp->datasets) (6.1.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.12.0 in e:\\software\\python\\lib\\site-packages (from aiohttp->datasets) (1.17.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in e:\\software\\python\\lib\\site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in e:\\software\\python\\lib\\site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in e:\\software\\python\\lib\\site-packages (from requests->transformers) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in e:\\software\\python\\lib\\site-packages (from requests->transformers) (2024.8.30)\n",
      "Requirement already satisfied: colorama in c:\\users\\vomtu\\appdata\\roaming\\python\\python312\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in e:\\software\\python\\lib\\site-packages (from jinja2->torch) (3.0.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\vomtu\\appdata\\roaming\\python\\python312\\site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in e:\\software\\python\\lib\\site-packages (from pandas->datasets) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in e:\\software\\python\\lib\\site-packages (from pandas->datasets) (2024.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\vomtu\\appdata\\roaming\\python\\python312\\site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in e:\\software\\python\\lib\\site-packages (from yarl<2.0,>=1.12.0->aiohttp->datasets) (0.2.0)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "\n",
    "%pip install transformers datasets torch torchvision evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ET3KtgrG8RkE"
   },
   "source": [
    " 2. Prepare the MNIST Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\software\\python\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Unrecognized instruction format: train[:0.1%]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m layers, models\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Tải bộ dữ liệu MNIST từ Hugging Face\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m train_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mload_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmnist\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msplit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrain[:0.1\u001b[39;49m\u001b[38;5;124;43m%\u001b[39;49m\u001b[38;5;124;43m]\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m test_dataset \u001b[38;5;241m=\u001b[39m load_dataset(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmnist\u001b[39m\u001b[38;5;124m\"\u001b[39m, split\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest[:0.1\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# Chuyển đổi dữ liệu thành định dạng TensorFlow\u001b[39;00m\n",
      "File \u001b[1;32me:\\software\\python\\Lib\\site-packages\\datasets\\load.py:2166\u001b[0m, in \u001b[0;36mload_dataset\u001b[1;34m(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, verification_mode, keep_in_memory, save_infos, revision, token, streaming, num_proc, storage_options, trust_remote_code, **config_kwargs)\u001b[0m\n\u001b[0;32m   2162\u001b[0m \u001b[38;5;66;03m# Build dataset for splits\u001b[39;00m\n\u001b[0;32m   2163\u001b[0m keep_in_memory \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m   2164\u001b[0m     keep_in_memory \u001b[38;5;28;01mif\u001b[39;00m keep_in_memory \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m is_small_dataset(builder_instance\u001b[38;5;241m.\u001b[39minfo\u001b[38;5;241m.\u001b[39mdataset_size)\n\u001b[0;32m   2165\u001b[0m )\n\u001b[1;32m-> 2166\u001b[0m ds \u001b[38;5;241m=\u001b[39m \u001b[43mbuilder_instance\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mas_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverification_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverification_mode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43min_memory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep_in_memory\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2167\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m save_infos:\n\u001b[0;32m   2168\u001b[0m     builder_instance\u001b[38;5;241m.\u001b[39m_save_infos()\n",
      "File \u001b[1;32me:\\software\\python\\Lib\\site-packages\\datasets\\builder.py:1126\u001b[0m, in \u001b[0;36mDatasetBuilder.as_dataset\u001b[1;34m(self, split, run_post_process, verification_mode, in_memory)\u001b[0m\n\u001b[0;32m   1123\u001b[0m verification_mode \u001b[38;5;241m=\u001b[39m VerificationMode(verification_mode \u001b[38;5;129;01mor\u001b[39;00m VerificationMode\u001b[38;5;241m.\u001b[39mBASIC_CHECKS)\n\u001b[0;32m   1125\u001b[0m \u001b[38;5;66;03m# Create a dataset for each of the given splits\u001b[39;00m\n\u001b[1;32m-> 1126\u001b[0m datasets \u001b[38;5;241m=\u001b[39m \u001b[43mmap_nested\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1127\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpartial\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1128\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_build_single_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1129\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrun_post_process\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_post_process\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1130\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverification_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverification_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1131\u001b[0m \u001b[43m        \u001b[49m\u001b[43min_memory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43min_memory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1132\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1133\u001b[0m \u001b[43m    \u001b[49m\u001b[43msplit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1134\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmap_tuple\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   1135\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdisable_tqdm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   1136\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1137\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(datasets, \u001b[38;5;28mdict\u001b[39m):\n\u001b[0;32m   1138\u001b[0m     datasets \u001b[38;5;241m=\u001b[39m DatasetDict(datasets)\n",
      "File \u001b[1;32me:\\software\\python\\Lib\\site-packages\\datasets\\utils\\py_utils.py:484\u001b[0m, in \u001b[0;36mmap_nested\u001b[1;34m(function, data_struct, dict_only, map_list, map_tuple, map_numpy, num_proc, parallel_min_length, batched, batch_size, types, disable_tqdm, desc)\u001b[0m\n\u001b[0;32m    482\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batched:\n\u001b[0;32m    483\u001b[0m     data_struct \u001b[38;5;241m=\u001b[39m [data_struct]\n\u001b[1;32m--> 484\u001b[0m mapped \u001b[38;5;241m=\u001b[39m \u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_struct\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    485\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batched:\n\u001b[0;32m    486\u001b[0m     mapped \u001b[38;5;241m=\u001b[39m mapped[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[1;32me:\\software\\python\\Lib\\site-packages\\datasets\\builder.py:1156\u001b[0m, in \u001b[0;36mDatasetBuilder._build_single_dataset\u001b[1;34m(self, split, run_post_process, verification_mode, in_memory)\u001b[0m\n\u001b[0;32m   1153\u001b[0m     split \u001b[38;5;241m=\u001b[39m Split(split)\n\u001b[0;32m   1155\u001b[0m \u001b[38;5;66;03m# Build base dataset\u001b[39;00m\n\u001b[1;32m-> 1156\u001b[0m ds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_as_dataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1157\u001b[0m \u001b[43m    \u001b[49m\u001b[43msplit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1158\u001b[0m \u001b[43m    \u001b[49m\u001b[43min_memory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43min_memory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1159\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1160\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m run_post_process:\n\u001b[0;32m   1161\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m resource_file_name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_post_processing_resources(split)\u001b[38;5;241m.\u001b[39mvalues():\n",
      "File \u001b[1;32me:\\software\\python\\Lib\\site-packages\\datasets\\builder.py:1230\u001b[0m, in \u001b[0;36mDatasetBuilder._as_dataset\u001b[1;34m(self, split, in_memory)\u001b[0m\n\u001b[0;32m   1228\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_legacy_cache():\n\u001b[0;32m   1229\u001b[0m     dataset_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname\n\u001b[1;32m-> 1230\u001b[0m dataset_kwargs \u001b[38;5;241m=\u001b[39m \u001b[43mArrowReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minfo\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1231\u001b[0m \u001b[43m    \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdataset_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1232\u001b[0m \u001b[43m    \u001b[49m\u001b[43minstructions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1233\u001b[0m \u001b[43m    \u001b[49m\u001b[43msplit_infos\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minfo\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplits\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1234\u001b[0m \u001b[43m    \u001b[49m\u001b[43min_memory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43min_memory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1235\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1236\u001b[0m fingerprint \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_dataset_fingerprint(split)\n\u001b[0;32m   1237\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m Dataset(fingerprint\u001b[38;5;241m=\u001b[39mfingerprint, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mdataset_kwargs)\n",
      "File \u001b[1;32me:\\software\\python\\Lib\\site-packages\\datasets\\arrow_reader.py:248\u001b[0m, in \u001b[0;36mBaseReader.read\u001b[1;34m(self, name, instructions, split_infos, in_memory)\u001b[0m\n\u001b[0;32m    227\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mread\u001b[39m(\n\u001b[0;32m    228\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    229\u001b[0m     name,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    232\u001b[0m     in_memory\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    233\u001b[0m ):\n\u001b[0;32m    234\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Returns Dataset instance(s).\u001b[39;00m\n\u001b[0;32m    235\u001b[0m \n\u001b[0;32m    236\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    245\u001b[0m \u001b[38;5;124;03m         kwargs to build a single Dataset instance.\u001b[39;00m\n\u001b[0;32m    246\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 248\u001b[0m     files \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_file_instructions\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minstructions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msplit_infos\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    249\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m files:\n\u001b[0;32m    250\u001b[0m         msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mInstruction \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00minstructions\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m corresponds to no data!\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "File \u001b[1;32me:\\software\\python\\Lib\\site-packages\\datasets\\arrow_reader.py:221\u001b[0m, in \u001b[0;36mBaseReader.get_file_instructions\u001b[1;34m(self, name, instruction, split_infos)\u001b[0m\n\u001b[0;32m    219\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_file_instructions\u001b[39m(\u001b[38;5;28mself\u001b[39m, name, instruction, split_infos):\n\u001b[0;32m    220\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return list of dict {'filename': str, 'skip': int, 'take': int}\"\"\"\u001b[39;00m\n\u001b[1;32m--> 221\u001b[0m     file_instructions \u001b[38;5;241m=\u001b[39m \u001b[43mmake_file_instructions\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    222\u001b[0m \u001b[43m        \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msplit_infos\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minstruction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfiletype_suffix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_filetype_suffix\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprefix_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_path\u001b[49m\n\u001b[0;32m    223\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    224\u001b[0m     files \u001b[38;5;241m=\u001b[39m file_instructions\u001b[38;5;241m.\u001b[39mfile_instructions\n\u001b[0;32m    225\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m files\n",
      "File \u001b[1;32me:\\software\\python\\Lib\\site-packages\\datasets\\arrow_reader.py:128\u001b[0m, in \u001b[0;36mmake_file_instructions\u001b[1;34m(name, split_infos, instruction, filetype_suffix, prefix_path)\u001b[0m\n\u001b[0;32m    117\u001b[0m name2filenames \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    118\u001b[0m     info\u001b[38;5;241m.\u001b[39mname: filenames_for_dataset_split(\n\u001b[0;32m    119\u001b[0m         path\u001b[38;5;241m=\u001b[39mprefix_path,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    125\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m info \u001b[38;5;129;01min\u001b[39;00m split_infos\n\u001b[0;32m    126\u001b[0m }\n\u001b[0;32m    127\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(instruction, ReadInstruction):\n\u001b[1;32m--> 128\u001b[0m     instruction \u001b[38;5;241m=\u001b[39m \u001b[43mReadInstruction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_spec\u001b[49m\u001b[43m(\u001b[49m\u001b[43minstruction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    129\u001b[0m \u001b[38;5;66;03m# Create the absolute instruction (per split)\u001b[39;00m\n\u001b[0;32m    130\u001b[0m absolute_instructions \u001b[38;5;241m=\u001b[39m instruction\u001b[38;5;241m.\u001b[39mto_absolute(name2len)\n",
      "File \u001b[1;32me:\\software\\python\\Lib\\site-packages\\datasets\\arrow_reader.py:564\u001b[0m, in \u001b[0;36mReadInstruction.from_spec\u001b[1;34m(cls, spec)\u001b[0m\n\u001b[0;32m    562\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m subs:\n\u001b[0;32m    563\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo instructions could be built out of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mspec\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 564\u001b[0m instruction \u001b[38;5;241m=\u001b[39m \u001b[43m_str_to_read_instruction\u001b[49m\u001b[43m(\u001b[49m\u001b[43msubs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    565\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msum\u001b[39m((_str_to_read_instruction(sub) \u001b[38;5;28;01mfor\u001b[39;00m sub \u001b[38;5;129;01min\u001b[39;00m subs[\u001b[38;5;241m1\u001b[39m:]), instruction)\n",
      "File \u001b[1;32me:\\software\\python\\Lib\\site-packages\\datasets\\arrow_reader.py:401\u001b[0m, in \u001b[0;36m_str_to_read_instruction\u001b[1;34m(spec)\u001b[0m\n\u001b[0;32m    399\u001b[0m res \u001b[38;5;241m=\u001b[39m _SUB_SPEC_RE\u001b[38;5;241m.\u001b[39mmatch(spec)\n\u001b[0;32m    400\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m res:\n\u001b[1;32m--> 401\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized instruction format: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mspec\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    402\u001b[0m unit \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m res\u001b[38;5;241m.\u001b[39mgroup(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfrom_pct\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m res\u001b[38;5;241m.\u001b[39mgroup(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mto_pct\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mabs\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    403\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ReadInstruction(\n\u001b[0;32m    404\u001b[0m     split_name\u001b[38;5;241m=\u001b[39mres\u001b[38;5;241m.\u001b[39mgroup(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msplit\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m    405\u001b[0m     rounding\u001b[38;5;241m=\u001b[39mres\u001b[38;5;241m.\u001b[39mgroup(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrounding\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    408\u001b[0m     unit\u001b[38;5;241m=\u001b[39munit,\n\u001b[0;32m    409\u001b[0m )\n",
      "\u001b[1;31mValueError\u001b[0m: Unrecognized instruction format: train[:0.1%]"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "\n",
    "# Tải bộ dữ liệu MNIST từ Hugging Face\n",
    "train_dataset = load_dataset(\"mnist\", split=\"train[:1%]\")\n",
    "test_dataset = load_dataset(\"mnist\", split=\"test[:1%]\")\n",
    "\n",
    "# Chuyển đổi dữ liệu thành định dạng TensorFlow\n",
    "def preprocess_function(example):\n",
    "    image = tf.keras.preprocessing.image.img_to_array(example['image'])  # PIL -> NumPy array\n",
    "    # Chuyển đổi hình ảnh từ đối tượng PIL thành TensorFlow tensor\n",
    "    image = tf.convert_to_tensor(image)  # Đảm bảo là tensor\n",
    "    image = tf.image.convert_image_dtype(image, tf.float32)  # Chuẩn hóa giá trị [0, 1]\n",
    "    image = tf.image.resize(image, (28, 28))  # Đảm bảo kích thước là 28x28\n",
    "    if image.shape[-1] != 1:  # Thêm kênh màu nếu chưa có (28, 28) -> (28, 28, 1)\n",
    "        image = tf.expand_dims(image, axis=-1)\n",
    "    label = tf.cast(example['label'], tf.int32)  # Đảm bảo nhãn là kiểu int\n",
    "    return {\"image\": image, \"label\": label}  # Trả về dạng dictionary với các cột\n",
    "\n",
    "# Áp dụng preprocessing cho tập huấn luyện và kiểm tra\n",
    "train_dataset = train_dataset.map(preprocess_function, remove_columns=[\"image\", \"label\"]).with_format(\"tensorflow\")\n",
    "test_dataset = test_dataset.map(preprocess_function, remove_columns=[\"image\", \"label\"]).with_format(\"tensorflow\")\n",
    "\n",
    "# Chuyển đổi thành tf.data.Dataset, thêm shuffle và batch\n",
    "# Chuyển đổi dataset Hugging Face thành TensorFlow Dataset\n",
    "train_dataset = tf.data.Dataset.from_generator(\n",
    "    lambda: iter(train_dataset), \n",
    "    output_signature=(\n",
    "        tf.TensorSpec(shape=(28, 28, 1), dtype=tf.float32),  # Kích thước và kiểu ảnh\n",
    "        tf.TensorSpec(shape=(), dtype=tf.int32)              # Kiểu nhãn\n",
    "    )\n",
    ")\n",
    "\n",
    "test_dataset = tf.data.Dataset.from_generator(\n",
    "    lambda: iter(test_dataset), \n",
    "    output_signature=(\n",
    "        tf.TensorSpec(shape=(28, 28, 1), dtype=tf.float32),\n",
    "        tf.TensorSpec(shape=(), dtype=tf.int32)\n",
    "    )\n",
    ")\n",
    "\n",
    "# Áp dụng các thao tác shuffle, batch và prefetch\n",
    "train_dataset = train_dataset.shuffle(1024).batch(32).prefetch(tf.data.AUTOTUNE)\n",
    "test_dataset = test_dataset.batch(32).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "# Kiểm tra dữ liệu\n",
    "for images, labels in train_dataset.take(1):\n",
    "    print(f\"Shape của batch ảnh: {images.shape}\")  # Kỳ vọng: (32, 28, 28, 1)\n",
    "    print(f\"Shape của batch nhãn: {labels.shape}\")  # Kỳ vọng: (32,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Load a Pretrained Vision Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224-in21k and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import ViTForImageClassification, ViTFeatureExtractor\n",
    "\n",
    "# Load feature extractor and model\n",
    "feature_extractor = ViTFeatureExtractor.from_pretrained(\"google/vit-base-patch16-224-in21k\")\n",
    "model = ViTForImageClassification.from_pretrained(\n",
    "    \"google/vit-base-patch16-224-in21k\",\n",
    "    num_labels=10,  # MNIST has 10 classes\n",
    "    id2label={i: str(i) for i in range(10)},  # Map IDs to digit labels\n",
    "    label2id={str(i): i for i in range(10)}\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Prepare the DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=32)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Fine-tune the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\oum1hc\\AppData\\Local\\Temp\\ipykernel_4192\\291651108.py:28: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  labels = torch.tensor(batch[\"label\"]).to(device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "Epoch 1 complete. Loss: 1.8444\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "Epoch 2 complete. Loss: 1.4378\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "Epoch 3 complete. Loss: 1.3000\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.optim import AdamW\n",
    "from transformers import get_scheduler\n",
    "\n",
    "# Define optimizer and scheduler\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "num_training_steps = len(train_dataloader) * 3  # Assuming 3 epochs\n",
    "lr_scheduler = get_scheduler(\"linear\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=num_training_steps)\n",
    "\n",
    "# Define loss function and move model to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Training loop\n",
    "model.train()\n",
    "for epoch in range(3):  # Train for 3 epochs\n",
    "    for batch in train_dataloader:\n",
    "        # Kiểm tra dữ liệu\n",
    "        #print(type(batch[\"image\"]))  # Kiểm tra kiểu dữ liệu\n",
    "\n",
    "        # Nếu batch[\"image\"] là list\n",
    "        if isinstance(batch[\"image\"], list):\n",
    "            inputs = torch.stack(batch[\"image\"]).to(device)\n",
    "        else:\n",
    "        # Nếu batch[\"image\"] đã là tensor\n",
    "            inputs = batch[\"image\"].to(device)\n",
    "\n",
    "        labels = torch.tensor(batch[\"label\"]).to(device)\n",
    "        outputs = model(pixel_values=inputs, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        \n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "\n",
    "\n",
    "    print(f\"Epoch {epoch + 1} complete. Loss: {loss.item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Evaluate the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\oum1hc\\AppData\\Local\\Temp\\ipykernel_4192\\3368052241.py:16: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  labels = torch.tensor(batch[\"label\"]).to(device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 80.00%\n"
     ]
    }
   ],
   "source": [
    "import evaluate\n",
    "\n",
    "# Load metric\n",
    "metric = evaluate.load(\"accuracy\")\n",
    "\n",
    "# Evaluation loop\n",
    "model.eval()\n",
    "for batch in test_dataloader:\n",
    "    \n",
    "    if isinstance(batch[\"image\"], list):\n",
    "        inputs = torch.stack(batch[\"image\"]).to(device)\n",
    "    else:\n",
    "        # Nếu batch[\"image\"] đã là tensor\n",
    "        inputs = batch[\"image\"].to(device)\n",
    "\n",
    "    labels = torch.tensor(batch[\"label\"]).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(pixel_values=inputs)\n",
    "    \n",
    "    logits = outputs.logits\n",
    "    predictions = torch.argmax(logits, dim=-1)\n",
    "    metric.add_batch(predictions=predictions, references=labels)\n",
    "\n",
    "# Compute final accuracy\n",
    "final_accuracy = metric.compute()[\"accuracy\"]\n",
    "print(f\"Test Accuracy: {final_accuracy * 100:.2f}%\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
